# SERL (Sample-Efficient Reinforcement Learning)

paper: https://arxiv.org/pdf/2401.16013

## Background

The paper wants to develop a real-world RL software package that anyone can use easily for robotic learning.

There are several desiderata:
(1) it must be efficient and able to make multiple gradient updates per time step.
(2) it must be able to incorporate prior data easily and then continue improving with further experience.
(3) it must be simple to debug and build on for new users.

SERL is built upon RLPD (Reinforcement Learning with prior data). RLPD: an off-policy Q-function actor-critic method that can readily incorporate prior data (either suboptimal data or demonstrations) into the replay buffer for efficient learning.
RLPD's feature: (i) high update-to-data ratio training (UTD), (ii) symmetric sampling between prior data and on-policy data, such that half of each batch comes from prior data and half from the online replay buffer, (iii) layer-norm regularization during training.

Each step of the algorithm updates the parameters of a parametric Q-function $Q_{\phi}(s, a)$ and actor $\pi_{\theta}(a|s)$ according to the gradient of their respective loss functions:

$$
L_Q(\phi) = E_{s, a, s'} \left[ \left( Q_\phi(s, a) - \left( r(s, a) + \gamma E_{a' \sim \pi_\theta} [Q_{\hat{\phi}}(s', a')] \right) \right)^2 \right]
$$

$$
L_\pi(\theta) = -E_{s} \left[ E_{a \sim \pi_\theta(a)} \left[ Q_\phi(s, a) \right] + \alpha \mathcal{H}(\pi_\theta(\cdot|s)) \right],
$$

where $Q_{\hat{\phi}}$ is a *target network*, and the actor loss uses entropy regularization with an adaptively adjusted weight $\alpha$. Each update step uses a sample-based approximation of each expectation, with half of the samples drawn from prior data (e.g., demonstrations), and half drawn from the *replay buffer*. For efficient learning, multiple update steps are performed per time step in the environment, which is referred to as the up-to-date (UTD) ratio, and regularizing the critic with layer normalization allows for higher UTD ratios and thus more efficient training.


## SERL Feature

### 1. **Reward Specification with Classifiers**
Defining reward functions manually can be challenging, particularly when learning from image observations. SERL employs a binary classifier to estimate the probability of a successful completion of a task based on the current state $s$. The reward function is defined as:

$$
r(s) = \log p(e|s),
$$

where $p(e|s)$ represents the classifier's output probability for the desired event $e$. This approach avoids the need for hand-specified rewards and allows the system to deduce task-specific rewards directly from data.

---

### 2. **Reset-Free Training with Forward-Backward Controllers**
For episodic tasks requiring the robot to reset after each attempt, SERL utilizes a **reset-free training** approach. This method employs two independent RL agents:
- The **forward agent**, which learns to perform the task.
- The **backward agent**, which learns to reset the environment to the initial state.

This dual-agent strategy reduces manual effort in resetting and ensures the robot can autonomously handle manipulation tasks, such as object relocation or assembly.

---

### 3. **Software Components for Adaptability**
SERL is designed to be compatible with various robotic environments. Key features include:
- A modular and adaptable environment wrapper for different robot configurations.
- Support for simulation-to-real (Sim2Real) transfer by integrating training with real-world data.
- High UTD (up-to-date) ratios for efficient policy optimization.

---

### 4. **Impedance Controller for Contact-Rich Tasks**
For tasks involving physical interactions, such as assembly or object manipulation, SERL integrates an impedance controller to ensure precise force and position control. The controller objective is defined as:

$$
F = k_p e + k_v \dot{e} + F_{f} + F_{cor},
$$

where:
- $e$ is the position error,
- $\dot{e}$ is the velocity error,
- $F_f$ is the feed-forward force,
- $F_{cor}$ is the Coriolis force.

This objective is converted into joint-space torques by multiplying the Jacobian transpose and offsetting with null-space torques. It acts like a spring-damper system around the equilibrium $p_{ref}$, with stiffness coefficient $k_p$ and damping coefficient $k_d$.

To avoid hard collisions or damage due to large forces, the system bounds $e$ such that $|e| \leq \Delta$. This ensures the force generated by the spring-damper system remains within safe limits:

$$
F = k_p \cdot |\Delta| + 2k_d \cdot |\Delta| \cdot f,
$$

where $f$ is the control frequency.

Directly clipping RL policy outputs might seem practical but is often inefficient for small-scale tasks, such as micrometer-level precision. Instead, SERL enforces constraints at the real-time control layer. This allows fast movement in free space while maintaining precise force limits during contact. This approach was tested on a Franka Panda robot and is easily implementable on other torque-controlled robots.

### 5. **Relative Observation and Action Frame**
The choice of action space significantly impacts RL training and generalization. SERL leverages a relative coordinate system for both observations and actions:
- Observations are expressed relative to the end-effector's initial pose.
- Actions (6D twist) are output relative to the current end-effector frame.

To simulate a dynamic target, the training procedure randomizes the robot's initial pose in each episode while keeping the target (e.g., PCB socket holes) fixed relative to the robot base frame. This is equivalent to physically moving the target when viewed from the end-effectorâ€™s frame, enabling the policy to adapt to perturbations or moving targets effectively.

