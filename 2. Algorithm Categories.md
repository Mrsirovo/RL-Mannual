# ALGORITHM CATEGORIES  

### **Model-Free vs Model-Based**  

- **Model-Free**:  
  Algorithms that **do not explicitly learn the environment’s dynamics** (transition probability $P$ and reward function $R$). Instead, they directly optimize value functions or policies based on interactions with the environment.  
  - **Examples**: Q-Learning, Policy Gradient, DQN, SAC.  

- **Model-Based**:  
  Algorithms that **learn a model of the environment** (i.e., $P(s'|s,a)$ and $R(s,a)$ ) and use it for planning or policy optimization.  
  - **Examples**: Dyna-Q, Monte Carlo Tree Search (MCTS), Model-Based PPO.  

---

### **Online vs Offline**  

- **Online Learning**:  
  Algorithms that **learn in real-time** by interacting with the environment. Data is collected **on-the-fly** and used immediately for learning.  
  - **Examples**: SARSA, Q-Learning, PPO.  

- **Offline Learning (Batch RL)**:  
  Algorithms that **learn from a fixed dataset** of experiences collected prior to training, without further interaction with the environment.  
  - **Examples**: Offline DQN, BCQ (Batch-Constrained Q-learning), CQL (Conservative Q-Learning).  

---

### **What to Learn**  

A key decision in RL algorithms is **what to learn**. This can include:  
- Policies (stochastic or deterministic).  
- Action-value functions ($Q$-functions).  
- Value functions.  
- Environment models.  

---

#### **What to Learn in Model-Free RL**  

1. **Policy Optimization**  
   - Represents a policy explicitly as $\pi_{\theta}(a|s)$.  
   - Optimizes $\theta$ directly via gradient ascent on the objective $J(\pi_{\theta})$ or indirectly by maximizing local approximations of $J(\pi_{\theta})$.  
   - Performed **on-policy**: updates use data collected while acting under the current policy.  
   - Often involves learning an approximator $V_{\phi}(s)$ for the on-policy value function $V^{\pi}(s)$ to assist in updating the policy.  

   **Examples**:  
   - **A2C / A3C**: Direct gradient ascent to maximize performance.  
   - **PPO**: Maximizes a surrogate objective to provide stable and conservative updates.  

2. **Q-Learning**  
   - Learns an approximator $Q_{\theta}(s,a)$ for the optimal action-value function $Q^*(s,a)$ using the Bellman equation.  
   - Performed **off-policy**: updates can use data collected at any point during training, independent of the current policy.  
   - The policy is derived via: $a(s) = \arg \max_a Q_{\theta}(s,a)$

   **Examples**:  
   - **DQN**: A classic deep RL algorithm that popularized Q-learning.  
   - **C51**: Learns a distribution over returns whose expectation is $Q^*$.  

#### **Trade-offs Between Policy Optimization and Q-Learning**  
- **Policy Optimization**:  
  - **Strengths**: Directly optimizes performance objective, stable and reliable.  
  - **Weaknesses**: Lower sample efficiency due to on-policy learning.  

- **Q-Learning**:  
  - **Strengths**: More sample efficient by reusing data effectively.  
  - **Weaknesses**: Indirectly optimizes performance, prone to instability.  

#### **Interpolating Between Policy Optimization and Q-Learning**  
Some algorithms combine elements of both approaches, trading off their respective strengths and weaknesses:  
- **DDPG**: Learns a deterministic policy and a Q-function simultaneously, using each to improve the other.  
- **SAC**: Uses stochastic policies, entropy regularization, and other techniques to stabilize learning and outperform DDPG on benchmarks.  

---

### **What to Learn in Model-Based RL**  

Unlike model-free RL, model-based RL has diverse methods for leveraging models. Here are a few notable approaches:  

- **Pure Planning**: The policy is not explicitly represented; instead, actions are chosen using planning techniques like **Model Predictive Control (MPC)**. At each step:  
  1. Compute an optimal plan over a fixed time window based on the model.  
  2. Execute the first action and discard the rest of the plan.  
  3. Re-plan at the next step to maintain a sufficiently long planning horizon.  
  - **Examples**: MBMF explores MPC with learned models for deep RL tasks.  

- **Expert Iteration (ExIt)**: Use a planning algorithm (e.g., Monte Carlo Tree Search) as an “expert” to generate better actions than the current policy $\pi_\theta(a|s)$. The policy is then updated to mimic the expert's output.  
  - **Examples**: ExIt trains deep networks to play Hex; AlphaZero uses this approach to master games like Chess and Go.  

- **Data Augmentation for Model-Free Methods**: Combine model-free RL with fictitious experiences generated by the model:  
  1. **Augment** real experiences with fictitious ones.  
  2. Use purely fictitious experiences for updates.  
  - **Examples**: MBVE augments real experiences; World Models train entirely on fictitious experiences (“training in the dream”).  

- **Embedding Planning Loops into Policies**: Embed planning procedures directly into the policy, allowing the policy to decide how and when to use the plans. The output is still trained using standard model-free algorithms. This approach reduces model bias by letting the policy **ignore bad model predictions** in certain states.  
  - **Examples**: I2A (Imagination-Augmented Agents) uses this style of imagination.  
