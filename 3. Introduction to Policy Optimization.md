# Introduction to Policy Optimization

## Deriving the Simplest Policy Gradient

Consider a stochastic, parameterized policy $\pi_{\theta}$. Our objective is to maximize the expected return $J(\pi_{\theta}) = E_{\tau \sim \pi_{\theta}} [R(\tau)]$. For this derivation, we'll assume $R(\tau)$ represents the finite-horizon undiscounted return, though the process for the infinite-horizon discounted return is similar.

To optimize the policy via gradient ascent:

$$
\theta_{k+1} = \theta_k + \alpha \nabla_{\theta} J(\pi_{\theta}) \big|_{\theta_k}
$$

The gradient of policy performance, $\nabla_{\theta} J(\pi_{\theta})$, is known as the policy gradient. Algorithms that optimize the policy in this manner are termed policy gradient algorithms (e.g., Vanilla Policy Gradient, TRPO).

To utilize this algorithm, we need an expression for the policy gradient that is computationally feasible. This involves two steps:

1. **Deriving the Analytical Gradient**: Determining the expected value form of the gradient.
2. **Forming a Sample Estimate**: Using data from a finite number of agent-environment interactions to approximate the expected value.

We'll begin by outlining a few useful facts for deriving the analytical gradient:

1. **Probability of a Trajectory**: The probability of a trajectory $\tau = (s_0, a_0, ..., s_{T+1})$, given actions from $\pi_{\theta}$, is:

$$
P(\tau|\theta) = \rho_0 (s_0) \prod_{t=0}^{T} P(s_{t+1}|s_t, a_t) \pi_{\theta}(a_t |s_t)
$$

3. **Log-Derivative Trick**: Based on calculus, the derivative of $\log x$ with respect to $x$ is $1/x$ ($\nabla x = x \nabla {\log x}$). Rearranging and applying the chain rule:

$$
\nabla_{\theta} P(\tau | \theta) = P(\tau | \theta) \nabla_{\theta} \log P(\tau | \theta)
$$

4. **Log-Probability of a Trajectory**: The log-probability of a trajectory is:

$$
\log P(\tau|\theta) = \log \rho_0 (s_0) + \sum_{t=0}^{T} \left( \log P(s_{t+1}|s_t, a_t) + \log \pi_{\theta}(a_t |s_t) \right)
$$

   Notably, the only term dependent on $\theta$ is $\log \pi_{\theta}(a_t |s_t)$.

$$
\log P(\tau|\theta) = \sum_{t=0}^{T} \log \pi_{\theta}(a_t |s_t)
$$

5. Using these facts, we can derive the policy gradient expression. The expected return is:

$$
J(\pi_{\theta}) = \int_{\tau} P(\tau|\theta) R(\tau)
$$

   Taking the gradient with respect to $\theta$:

$$
\nabla_{\theta} J(\pi_{\theta}) = \int_{\tau} \nabla_{\theta} P(\tau|\theta) R(\tau)
$$

6. Thus, the policy gradient becomes:

$$
\nabla_{\theta} J(\pi_{\theta}) = E_{\tau | \pi_{\theta}} \left[ R(\tau) \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \right]
$$

   This is the simplest form of the policy gradient.

This is an expectation, which means that we can estimate it with a sample mean. If we collect a set of trajectories $D = {\tau_i}, i=1,...,N$, where each trajectory is obtained by letting the agent act in the environment using the policy $\pi_{\theta}$, the policy gradient can be estimated with:

$$
\hat{g} = \frac{1}{|D|} \sum_{\tau \in D} \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) R(\tau),
$$

where $|\mathcal{D}|$ is the number of trajectories in $D$ (here, $N$).

This expression is the simplest version of the computable policy gradient. Assuming that the policy is represented in a way that allows us to compute $\nabla_{\theta} \log \pi_{\theta}(a|s)$ and that we can run the policy in the environment to collect the trajectory dataset, we can compute the policy gradient and take an update step.

## Implementing the Simplest Policy Gradient

This is the simplest computable form of the policy gradient. To implement it:

1. **Policy Representation**: Represent the policy in a way that allows computation of $\nabla_{\theta} \log \pi_{\theta}(a|s)$.

2. **Data Collection**: Collect trajectory data by running the policy in the environment.

3. **Gradient Computation and Update**: Use the dataset to compute the policy gradient and update the policy parameters $\theta$.

For a practical implementation, consider the following Python code using TensorFlow:

```python
# make core of policy network
logits_net = mlp(sizes=[obs_dim]+hidden_sizes+[n_acts])

# make function to compute action distribution
def get_policy(obs):
    logits = logits_net(obs)
    return Categorical(logits=logits)

# make action selection function (outputs int actions, sampled from policy)
def get_action(obs):
    return get_policy(obs).sample().item()

# make loss function whose gradient, for the right data, is policy gradient
def compute_loss(obs, act, weights):
    logp = get_policy(obs).log_prob(act)
    return -(logp * weights).mean()

# for training policy
def train_one_epoch():
    # make some empty lists for logging.
    batch_obs = []          # for observations
    batch_acts = []         # for actions
    batch_weights = []      # for R(tau) weighting in policy gradient
    batch_rets = []         # for measuring episode returns
    batch_lens = []         # for measuring episode lengths

    # reset episode-specific variables
    obs = env.reset()       # first obs comes from starting distribution
    done = False            # signal from environment that episode is over
    ep_rews = []            # list for rewards accrued throughout ep

    # render first episode of each epoch
    finished_rendering_this_epoch = False

    # collect experience by acting in the environment with current policy
    while True:

        # rendering
        if (not finished_rendering_this_epoch) and render:
            env.render()

        # save obs
        batch_obs.append(obs.copy())

        # act in the environment
        act = get_action(torch.as_tensor(obs, dtype=torch.float32))
        obs, rew, done, _ = env.step(act)

        # save action, reward
        batch_acts.append(act)
        ep_rews.append(rew)

        if done:
            # if episode is over, record info about episode
            ep_ret, ep_len = sum(ep_rews), len(ep_rews)
            batch_rets.append(ep_ret)
            batch_lens.append(ep_len)

            # the weight for each logprob(a|s) is R(tau)
            batch_weights += [ep_ret] * ep_len

            # reset episode-specific variables
            obs, done, ep_rews = env.reset(), False, []

            # won't render again this epoch
            finished_rendering_this_epoch = True

            # end experience loop if we have enough of it
            if len(batch_obs) > batch_size:
                break

    # take a single policy gradient update step
    optimizer.zero_grad()
    batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),
                              act=torch.as_tensor(batch_acts, dtype=torch.int32),
                              weights=torch.as_tensor(batch_weights, dtype=torch.float32)
                              )
    batch_loss.backward()
    optimizer.step()
    return batch_loss, batch_rets, batch_lens


