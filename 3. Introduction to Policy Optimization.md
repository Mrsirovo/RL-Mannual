# Introduction to Policy Optimization

## Deriving the Simplest Policy Gradient

Consider a stochastic, parameterized policy $\pi_{\theta}$. Our objective is to maximize the expected return $J(\pi_{\theta}) = E_{\tau \sim \pi_{\theta}} [R(\tau)]$. For this derivation, we'll assume $R(\tau)$ represents the finite-horizon undiscounted return, though the process for the infinite-horizon discounted return is similar.

To optimize the policy via gradient ascent:

$\theta_{k+1} = \theta_k + \alpha \nabla_{\theta} J(\pi_{\theta}) \big|_{\theta_k}$

The gradient of policy performance, $\nabla_{\theta} J(\pi_{\theta})$, is known as the policy gradient. Algorithms that optimize the policy in this manner are termed policy gradient algorithms (e.g., Vanilla Policy Gradient, TRPO).

To utilize this algorithm, we need an expression for the policy gradient that is computationally feasible. This involves two steps:

1. **Deriving the Analytical Gradient**: Determining the expected value form of the gradient.
2. **Forming a Sample Estimate**: Using data from a finite number of agent-environment interactions to approximate the expected value.

We'll begin by outlining a few useful facts for deriving the analytical gradient:

1. **Probability of a Trajectory**: The probability of a trajectory $\tau = (s_0, a_0, ..., s_{T+1})$, given actions from $\pi_{\theta}$, is:

    $P(\tau|\theta) = \rho_0 (s_0) \prod_{t=0}^{T} P(s_{t+1}|s_t, a_t) \pi_{\theta}(a_t |s_t)$

2. **Log-Derivative Trick**: Based on calculus, the derivative of $\log x$ with respect to $x$ is $1/x$ ($\nabla x = x \nabla {\log x}$). Rearranging and applying the chain rule:

    $\nabla_{\theta} P(\tau | \theta) = P(\tau | \theta) \nabla_{\theta} \log P(\tau | \theta)$

3. **Log-Probability of a Trajectory**: The log-probability of a trajectory is:

    $\log P(\tau|\theta) = \log \rho_0 (s_0) + \sum_{t=0}^{T} \left( \log P(s_{t+1}|s_t, a_t) + \log \pi_{\theta}(a_t |s_t) \right)$

   Notably, the only term dependent on $\theta$ is $\log \pi_{\theta}(a_t |s_t)$.

   $\log P(\tau|\theta) = \sum_{t=0}^{T} \log \pi_{\theta}(a_t |s_t)$

4. Using these facts, we can derive the policy gradient expression. The expected return is:

   $J(\pi_{\theta}) = \int_{\tau} P(\tau|\theta) R(\tau)$

   Taking the gradient with respect to $\theta$:

   $\nabla_{\theta} J(\pi_{\theta}) = \int_{\tau} \nabla_{\theta} P(\tau|\theta) R(\tau)$

5. Thus, the policy gradient becomes:

   $\nabla_{\theta} J(\pi_{\theta}) = E_{\tau | \pi_{\theta}} \left[ R(\tau) \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \right]$

   This is the simplest form of the policy gradient.

This is an expectation, which means that we can estimate it with a sample mean. If we collect a set of trajectories $D = {\tau_i}, i=1,...,N$, where each trajectory is obtained by letting the agent act in the environment using the policy $\pi_{\theta}$, the policy gradient can be estimated with:

$\hat{g} = \frac{1}{|D|} \sum_{\tau \in D} \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) R(\tau),$

where $|\mathcal{D}|$ is the number of trajectories in $D$ (here, $N$).

This expression is the simplest version of the computable policy gradient. Assuming that the policy is represented in a way that allows us to compute $\nabla_{\theta} \log \pi_{\theta}(a|s)$ and that we can run the policy in the environment to collect the trajectory dataset, we can compute the policy gradient and take an update step.

## Implementing the Simplest Policy Gradient

To implement this gradient, we approximate the expectation with samples. Collect a set of trajectories $\{\tau_i\}$ by executing the current policy $\pi_{\theta}$ in the environment. The gradient estimate is:

$\nabla_{\theta} J(\pi_{\theta}) \approx \frac{1}{N} \sum_{i=1}^{N} \left[ R(\tau_i) \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t^i |s_t^i) \right]$

Where $N$ is the number of sampled trajectories. This forms the basis for the REINFORCE algorithm.
