# Introduction to Policy Optimization

Policy optimization involves adjusting an agent's policy to maximize expected returns. This process is central to reinforcement learning (RL) and is the foundation for various algorithms.

## Policy Gradient Theorem  

The **Policy Gradient Theorem** provides a method to compute the gradient of the expected return with respect to policy parameters. For a parameterized stochastic policy $\pi_{\theta}(a|s)$, the objective is to maximize the expected return $J(\pi_{\theta})$ :

$J(\pi_{\theta}) = E_{\tau \sim \pi_{\theta}} [R(\tau)]$

where $R(\tau)$ denotes the return of trajectory $\tau$. The gradient of $J(\pi_{\theta})$ is given by:

$\nabla_{\theta} J(\pi_{\theta}) = E_{\tau \sim \pi_{\theta}} \left[ \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) R(\tau) \right]$

This expression allows for gradient ascent to optimize the policy parameters.

## Sample-Based Estimation  

In practice, the expectation is approximated using samples. By collecting a set of trajectories $\{\tau_i\}$ through interactions with the environment, the policy gradient can be estimated as:

$\nabla_{\theta} J(\pi_{\theta}) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t^i | s_t^i) R(\tau_i)$

where $N$ is the number of sampled trajectories.

## Variance Reduction Techniques  

To improve the efficiency of policy gradient methods, variance reduction techniques are employed:

- **Reward-to-Go**: Instead of using the total return $R(\tau)$, utilize the cumulative future rewards from time step $t$ onwards, denoted as $R_t$. This approach refines the gradient estimate by focusing on rewards attributable to specific actions.

- **Baselines**: Introduce a baseline function $b(s_t)$ to the advantage function to reduce variance without introducing bias. The modified gradient estimate becomes:

$\nabla_{\theta} J(\pi_{\theta}) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t^i | s_t^i) \left( R_t^i - b(s_t^i) \right)$

A common choice for $b(s_t)$ is the state value function $V^{\pi}(s_t)$.

## Advantage Function  

The **Advantage Function** $A^{\pi}(s, a)$ quantifies the relative value of an action compared to the average action at a state:

$A^{\pi}(s, a) = Q^{\pi}(s, a) - V^{\pi}(s)$

Incorporating the advantage function into the policy gradient enhances learning by emphasizing actions that yield higher-than-expected returns.

## Algorithms  

Several algorithms are built upon these principles:

- **Vanilla Policy Gradient (VPG)**: Also known as REINFORCE, it updates policy parameters in the direction of the estimated gradient.  

- **Trust Region Policy Optimization (TRPO)**: Introduces constraints to ensure policy updates do not deviate significantly, enhancing stability.  

- **Proximal Policy Optimization (PPO)**: Simplifies TRPO by using a clipped objective function to balance exploration and exploitation.  

These algorithms iteratively adjust the policy to improve performance in various RL tasks.
